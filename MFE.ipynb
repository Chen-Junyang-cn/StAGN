{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f77e487",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T13:10:48.464162Z",
     "iopub.status.busy": "2022-05-30T13:10:48.463729Z",
     "iopub.status.idle": "2022-05-30T13:11:29.029152Z",
     "shell.execute_reply": "2022-05-30T13:11:29.028260Z"
    },
    "papermill": {
     "duration": 40.600936,
     "end_time": "2022-05-30T13:11:29.045590",
     "exception": false,
     "start_time": "2022-05-30T13:10:48.444654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data successfully\n",
      "Number of samples:  8589\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pywt\n",
    "import sys\n",
    "sys.path.append('../input')\n",
    "\n",
    "from isrucutitls.Utils import *\n",
    "from isrucutitls.DataGenerator import *\n",
    "\n",
    "Path = \"../input/isruc-s3-preprocess/ISRUC_S3_all.npz\"\n",
    "# Path = \"../input/isruc-s3-wavelet-process/ISRUC_S3_wavelet_process.npz\"\n",
    "ReadList = np.load(Path, allow_pickle=True)\n",
    "Fold_Num   = ReadList['Fold_len']    # Num of samples of each fold\n",
    "Fold_Data  = ReadList['Fold_data']   # Data of each fold\n",
    "Fold_Label = ReadList['Fold_label']  # Labels of each fold\n",
    "\n",
    "print(\"Read data successfully\")\n",
    "print('Number of samples: ',np.sum(Fold_Num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd12dc96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T13:11:29.079780Z",
     "iopub.status.busy": "2022-05-30T13:11:29.079519Z",
     "iopub.status.idle": "2022-05-30T13:11:29.083815Z",
     "shell.execute_reply": "2022-05-30T13:11:29.082964Z"
    },
    "papermill": {
     "duration": 0.023437,
     "end_time": "2022-05-30T13:11:29.085585",
     "exception": false,
     "start_time": "2022-05-30T13:11:29.062148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DataGenerator = kFoldGenerator(Fold_Data, Fold_Label)\n",
    "Dom_Generator = DominGenerator(Fold_Num)\n",
    "del Fold_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec628b02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T13:11:29.119962Z",
     "iopub.status.busy": "2022-05-30T13:11:29.119284Z",
     "iopub.status.idle": "2022-05-30T13:11:29.128670Z",
     "shell.execute_reply": "2022-05-30T13:11:29.127896Z"
    },
    "papermill": {
     "duration": 0.02874,
     "end_time": "2022-05-30T13:11:29.130677",
     "exception": false,
     "start_time": "2022-05-30T13:11:29.101937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([924, 911, 794, 764, 914, 823, 784, 970, 939, 766], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fold_Num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4986a537",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T13:11:29.165565Z",
     "iopub.status.busy": "2022-05-30T13:11:29.164822Z",
     "iopub.status.idle": "2022-05-30T13:11:29.170420Z",
     "shell.execute_reply": "2022-05-30T13:11:29.169671Z"
    },
    "papermill": {
     "duration": 0.025076,
     "end_time": "2022-05-30T13:11:29.172278",
     "exception": false,
     "start_time": "2022-05-30T13:11:29.147202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_domin, val_domin = Dom_Generator.getFold(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbf1b475",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T13:11:29.207583Z",
     "iopub.status.busy": "2022-05-30T13:11:29.206800Z",
     "iopub.status.idle": "2022-05-30T13:11:29.219465Z",
     "shell.execute_reply": "2022-05-30T13:11:29.218591Z"
    },
    "papermill": {
     "duration": 0.032361,
     "end_time": "2022-05-30T13:11:29.221418",
     "exception": false,
     "start_time": "2022-05-30T13:11:29.189057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([924, 794, 764, 914, 823, 784, 970, 939, 766])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_domin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05fbd606",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T13:11:29.256856Z",
     "iopub.status.busy": "2022-05-30T13:11:29.256577Z",
     "iopub.status.idle": "2022-05-30T13:11:29.262921Z",
     "shell.execute_reply": "2022-05-30T13:11:29.262184Z"
    },
    "papermill": {
     "duration": 0.026316,
     "end_time": "2022-05-30T13:11:29.264957",
     "exception": false,
     "start_time": "2022-05-30T13:11:29.238641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(val_domin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e37ac77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T13:11:29.319793Z",
     "iopub.status.busy": "2022-05-30T13:11:29.319410Z",
     "iopub.status.idle": "2022-05-30T13:11:38.298390Z",
     "shell.execute_reply": "2022-05-30T13:11:38.297277Z"
    },
    "papermill": {
     "duration": 9.01075,
     "end_time": "2022-05-30T13:11:38.300301",
     "exception": false,
     "start_time": "2022-05-30T13:11:29.289551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras version: 2.6.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16031863231458860711\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15696920576\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2672983201434185995\n",
      "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 13:11:32.540803: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-30 13:11:32.611321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:11:32.612619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:11:32.613587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:11:37.954884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:11:37.955794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:11:37.956437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:11:37.957711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 14969 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "2022-05-30 13:11:37.963617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:11:37.964278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:11:37.964940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:11:37.965918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:11:37.966566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:11:37.967167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:11:37.967900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:11:37.968736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:11:37.969294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14969 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "# !pip install wfdb\n",
    "# import wfdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.interpolate import splev, splrep\n",
    "import pickle\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# !pip install biosppy\n",
    "# import biosppy.signals.tools as st\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, Flatten, MaxPooling1D, Activation,\\\n",
    "BatchNormalization, Add, Reshape, TimeDistributed, Input, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "# !pip install torch\n",
    "import torch\n",
    "# import torch.nn as nn\n",
    "# !pip install torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio\n",
    "# !pip install librosa\n",
    "# import librosa\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, scale\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "from tqdm.auto import tqdm\n",
    "from tensorflow.python.client import device_lib\n",
    "import pywt\n",
    "\n",
    "import random as python_random\n",
    "np.random.seed(4242)\n",
    "python_random.seed(4242)\n",
    "tf.random.set_seed(4242)\n",
    "print(\"keras version:\", keras.__version__)\n",
    " \n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "# tf.config.experimental.list_physical_devices('CPU')\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "# AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# # Create strategy from tpu\n",
    "# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "# tf.config.experimental_connect_to_cluster(tpu)\n",
    "# tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "# strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0f939cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T13:11:38.338902Z",
     "iopub.status.busy": "2022-05-30T13:11:38.338092Z",
     "iopub.status.idle": "2022-05-30T13:11:38.346638Z",
     "shell.execute_reply": "2022-05-30T13:11:38.345774Z"
    },
    "papermill": {
     "duration": 0.030676,
     "end_time": "2022-05-30T13:11:38.348598",
     "exception": false,
     "start_time": "2022-05-30T13:11:38.317922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CNN(inputs, fs=32, kernel_size=25, pool_size=16, weight=0.001):\n",
    "    x = Conv1D(fs, kernel_size,1, padding='same', kernel_regularizer=l2(weight))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(pool_size, 2, padding='same')(x)\n",
    "    return x\n",
    "\n",
    "def ResNet(inputs, fs=32, ks_1=25, ps_1=16, ks_2=25, ps_2=16, weight=0.001):\n",
    "    x = CNN(inputs, fs, ks_1, ps_1,weight)\n",
    "    x = CNN(x, fs, ks_2, ps_2,weight)\n",
    "    shortcut_x = Conv1D(fs,1,2,padding='same')(inputs)\n",
    "    shortcut_x = Conv1D(fs,1,2,padding='same')(shortcut_x)\n",
    "    return Add()([x, shortcut_x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfaedc7",
   "metadata": {
    "papermill": {
     "duration": 0.018562,
     "end_time": "2022-05-30T13:11:38.385524",
     "exception": false,
     "start_time": "2022-05-30T13:11:38.366962",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78beea7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T13:11:38.424482Z",
     "iopub.status.busy": "2022-05-30T13:11:38.424127Z",
     "iopub.status.idle": "2022-05-30T13:11:38.435882Z",
     "shell.execute_reply": "2022-05-30T13:11:38.434954Z"
    },
    "papermill": {
     "duration": 0.034155,
     "end_time": "2022-05-30T13:11:38.438350",
     "exception": false,
     "start_time": "2022-05-30T13:11:38.404195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 注意睡眠期 N1，N2，N3 可能很相似，因此需要调低 温度系数\n",
    "temperature = 0.07\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "def multi_label_supconloss(features, labels):\n",
    "    \"\"\"\n",
    "    :param features: (batch_size, feature_dim)\n",
    "    :param labels: (batch_size, label_dim), labels should be one-hot vector\n",
    "    :return: loss\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(features)[0]\n",
    "    features = K.l2_normalize(K.cast(features, dtype=tf.float64), axis=1)\n",
    "\n",
    "    # compute the cosine similarity, we assume the features were normalized after embedding\n",
    "    logit = -K.dot(features, K.transpose(features)) / temperature # (batch_size, batch_size)\n",
    "    # for numerical stability\n",
    "    logits_max = K.max(logit, axis=1, keepdims=True)\n",
    "    logit = logit - logits_max\n",
    "    # compute the euclidean distance\n",
    "    # logit = K.sum(tf.square(features), axis=1, keepdims=True) - 2 * K.dot(features, K.transpose(features)) + \\\n",
    "    #         K.transpose(K.sum(K.square(features), axis=1, keepdims=True))  # (batch_size, batch_size)\n",
    "    # logit = K.sqrt(K.cast(logit, dtype=tf.float64)) / temperature\n",
    "\n",
    "    # mask self-contrast similarity\n",
    "    # logit_mask = K.ones((batch_size, batch_size), dtype=tf.bool) - K.eye(batch_size, dtype=tf.bool)\n",
    "    logit_mask = K.cast(~tf.eye(batch_size, dtype=tf.bool), dtype=tf.float64)\n",
    "    exp_logit = K.exp(logit) * logit_mask\n",
    "\n",
    "    log_prob = logit - K.log(K.sum(exp_logit, axis=1, keepdims=True))\n",
    "    \n",
    "    # compute the weight for each pair(i,j)'s similarity\n",
    "    weight = K.cast(K.dot(labels, K.transpose(labels)), dtype=tf.float64)\n",
    "    # mask\n",
    "    weight_mask = K.cast(~tf.eye(batch_size, dtype=bool), dtype=tf.float64)\n",
    "    weight = weight * weight_mask\n",
    "    # weight = -weight / K.sum(weight, axis=1, keepdims=True)\n",
    "    total_weight = K.sum(weight, axis=1, keepdims=True)\n",
    "    weight = tf.where(total_weight != 0., weight / total_weight, weight)\n",
    "    loss = K.mean(K.cast(weight, dtype=tf.float64) * log_prob)\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0f822e",
   "metadata": {
    "papermill": {
     "duration": 0.018319,
     "end_time": "2022-05-30T13:11:38.475331",
     "exception": false,
     "start_time": "2022-05-30T13:11:38.457012",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc0a5332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T13:11:38.513988Z",
     "iopub.status.busy": "2022-05-30T13:11:38.513720Z",
     "iopub.status.idle": "2022-05-30T13:11:39.555485Z",
     "shell.execute_reply": "2022-05-30T13:11:39.554162Z"
    },
    "papermill": {
     "duration": 1.064558,
     "end_time": "2022-05-30T13:11:39.557890",
     "exception": false,
     "start_time": "2022-05-30T13:11:38.493332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 3000, 16)     416         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 3000, 16)     64          conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 3000, 16)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1500, 16)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1500, 16)     6416        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1500, 16)     64          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1500, 16)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1500, 16)     32          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 750, 16)      0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 750, 16)      272         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 750, 16)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 750, 16)      0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 750, 32)      12832       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 750, 32)      128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 750, 32)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 375, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 375, 32)      25632       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 375, 32)      128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 375, 32)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 375, 32)      544         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 188, 32)      0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 188, 32)      1056        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 188, 32)      0           max_pooling1d_3[0][0]            \n",
      "                                                                 conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 188, 32)      0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 188, 64)      51264       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 188, 64)      256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 188, 64)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 94, 64)       0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 94, 64)       102464      max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 94, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 94, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 94, 64)       2112        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 47, 64)       0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 47, 64)       4160        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 47, 64)       0           max_pooling1d_5[0][0]            \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 47, 64)       0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 47, 128)      204928      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 47, 128)      512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 47, 128)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 24, 128)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 24, 128)      409728      max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 24, 128)      512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 24, 128)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 24, 128)      8320        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 12, 128)      0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 12, 128)      16512       conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 12, 128)      0           max_pooling1d_7[0][0]            \n",
      "                                                                 conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 848,608\n",
      "Trainable params: 847,648\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 3000, 10)]        0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 10, 3000, 1)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 10, 128)           848608    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "Feature (Dense)              (None, 128)               163968    \n",
      "=================================================================\n",
      "Total params: 1,012,576\n",
      "Trainable params: 1,011,616\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 3000, 10)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 10, 3000, 1)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 10, 128)      848608      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1280)         0           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1280)         0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Feature (Dense)                 (None, 128)          163968      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gradient_reversal (GradientReve (None, 128)          1           Feature[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Label (Dense)                   (None, 5)            645         Feature[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Domain (Dense)                  (None, 9)            1161        gradient_reversal[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 1,014,383\n",
      "Trainable params: 1,013,423\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 3000, 10)]        0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 10, 3000, 1)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 10, 128)           848608    \n",
      "=================================================================\n",
      "Total params: 848,608\n",
      "Trainable params: 847,648\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 3000, 10)]        0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 10, 3000, 1)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 10, 128)           848608    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "Feature (Dense)              (None, 128)               163968    \n",
      "_________________________________________________________________\n",
      "Label (Dense)                (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,013,221\n",
      "Trainable params: 1,012,261\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class GradientReversal(keras.layers.Layer):\n",
    "    '''Flip the sign of gradient during training.'''\n",
    "\n",
    "    @tf.custom_gradient\n",
    "    def grad_reverse(self, x):\n",
    "        y = tf.identity(x)\n",
    "        def custom_grad(dy):\n",
    "            return -self.hp_lambda * dy\n",
    "        return y, custom_grad\n",
    "\n",
    "    # --------------------------------------\n",
    "    def __init__(self, hp_lambda=0.0001, **kwargs):\n",
    "        super(GradientReversal, self).__init__(**kwargs)\n",
    "        self.hp_lambda = K.variable(hp_lambda, dtype='float32', name='hp_lambda')\n",
    "\n",
    "    # --------------------------------------\n",
    "    def call(self, x, mask=None):\n",
    "        return self.grad_reverse(x)\n",
    "\n",
    "    # --------------------------------------\n",
    "    def set_hp_lambda(self,hp_lambda):\n",
    "        #self.hp_lambda = hp_lambda\n",
    "        K.set_value(self.hp_lambda, hp_lambda)\n",
    "\n",
    "    # --------------------------------------\n",
    "    def increment_hp_lambda_by(self,increment):\n",
    "        new_value = float(K.get_value(self.hp_lambda)) +  increment\n",
    "        K.set_value(self.hp_lambda, new_value)\n",
    "\n",
    "    def get_hp_lambda(self):\n",
    "        return float(K.get_value(self.hp_lambda))\n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        base_config = super(GradientReversal, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def create_model(input_shape, channels=10, time_second=30, freq=100,num_domain=9):\n",
    "    inputs_channel = Input(shape=(time_second*freq, 1))\n",
    "    x = ResNet(inputs_channel, 16)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = ResNet(x, 32)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = ResNet(x, 64)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = ResNet(x, 128)\n",
    "#     x = tf.keras.layers.MultiHeadAttention(num_heads=2,key_dim=2)(x, x)\n",
    "#     x = ResNet(x, 10, 6, 6, 4)\n",
    "#     x = ResNet(x, 5, 3, 4, 2)\n",
    "#     x = Flatten()(x)\n",
    "#     outputs = Dense(128)(x)\n",
    "    outputs = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    fea_part = Model(inputs=inputs_channel, outputs=outputs)\n",
    "    fea_part.summary()\n",
    "    \n",
    "    # extract the features from each channel\n",
    "    inputs = Input(shape=input_shape)\n",
    "    input_re = Reshape((channels, time_second * freq, 1))(inputs)\n",
    "#     fea_all = tf.stack([fea_part(input_re[:,i,:,:]) for i in range(channels)], axis=1)\n",
    "    fea_all = TimeDistributed(fea_part)(input_re)\n",
    "#     fea_all = tf.keras.layers.Attention(use_scale=True)([fea_all, fea_all])\n",
    "    \n",
    "    fla_fea = Flatten()(fea_all)\n",
    "    fla_fea = Dropout(0.5)(fla_fea)\n",
    "#     merged = GlobalAveragePooling1D()(fea_all)\n",
    "    merged = Dense(128, name='Feature')(fla_fea)\n",
    "    label_out = Dense(5, activation='softmax', name='Label')(merged)\n",
    "    # GRL \n",
    "    GRL_in = GradientReversal()(merged)\n",
    "#     dense_out = Dense(128)(GRL_in)\n",
    "    domain_out = Dense(num_domain, activation='softmax', name='Domain')(GRL_in)\n",
    "    \n",
    "    fea_model = Model(inputs, fea_all)\n",
    "    cl_model = Model(inputs, merged)\n",
    "    ce_model = Model(inputs, [label_out, domain_out, merged])\n",
    "    pre_model = Model(inputs, label_out)\n",
    "    \n",
    "    return fea_model, cl_model, ce_model, pre_model\n",
    "\n",
    "\n",
    "def model_test():\n",
    "    train_data, val_data = [np.random.rand(1,3000,10), np.random.rand(1,3000,10)]\n",
    "    fea_model, cl_model, ce_model, pre_model = create_model(train_data.shape[1:])\n",
    "    cl_model.summary()\n",
    "    ce_model.summary()\n",
    "    fea_model.summary()\n",
    "    pre_model.summary()\n",
    "model_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcba46ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T13:11:39.599645Z",
     "iopub.status.busy": "2022-05-30T13:11:39.599358Z",
     "iopub.status.idle": "2022-05-30T16:48:32.596973Z",
     "shell.execute_reply": "2022-05-30T16:48:32.596082Z"
    },
    "papermill": {
     "duration": 13013.861216,
     "end_time": "2022-05-30T16:48:33.439343",
     "exception": false,
     "start_time": "2022-05-30T13:11:39.578127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold # 0\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 3000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 3000, 16)     416         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 3000, 16)     64          conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 3000, 16)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1500, 16)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 1500, 16)     6416        max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 1500, 16)     64          conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1500, 16)     0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 1500, 16)     32          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 750, 16)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 750, 16)      272         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 750, 16)      0           max_pooling1d_9[0][0]            \n",
      "                                                                 conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 750, 16)      0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 750, 32)      12832       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 750, 32)      128         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 750, 32)      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 375, 32)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 375, 32)      25632       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 375, 32)      128         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 375, 32)      0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 375, 32)      544         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 188, 32)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 188, 32)      1056        conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 188, 32)      0           max_pooling1d_11[0][0]           \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 188, 32)      0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 188, 64)      51264       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 188, 64)      256         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 188, 64)      0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 94, 64)       0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 94, 64)       102464      max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 94, 64)       256         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 94, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 94, 64)       2112        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 47, 64)       0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 47, 64)       4160        conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 47, 64)       0           max_pooling1d_13[0][0]           \n",
      "                                                                 conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 47, 64)       0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 47, 128)      204928      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 47, 128)      512         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 47, 128)      0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 24, 128)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 24, 128)      409728      max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 24, 128)      512         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 24, 128)      0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 24, 128)      8320        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 12, 128)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 12, 128)      16512       conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 12, 128)      0           max_pooling1d_15[0][0]           \n",
      "                                                                 conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           add_7[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 848,608\n",
      "Trainable params: 847,648\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 13:11:44.098065: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 919800000 exceeds 10% of free system memory.\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:4212: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n",
      "2022-05-30 13:11:45.284315: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 919800000 exceeds 10% of free system memory.\n",
      "2022-05-30 13:11:46.065686: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 13:11:47.038974: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 38s 493ms/step - loss: 0.1544 - val_loss: 0.0941\n",
      "Epoch 2/15\n",
      "60/60 [==============================] - 27s 450ms/step - loss: 0.0749 - val_loss: 0.0876\n",
      "Epoch 3/15\n",
      "60/60 [==============================] - 28s 468ms/step - loss: 0.0703 - val_loss: 0.0843\n",
      "Epoch 4/15\n",
      "60/60 [==============================] - 28s 468ms/step - loss: 0.0650 - val_loss: 0.0748\n",
      "Epoch 5/15\n",
      "60/60 [==============================] - 28s 469ms/step - loss: 0.1157 - val_loss: 0.1613\n",
      "Epoch 6/15\n",
      "60/60 [==============================] - 28s 467ms/step - loss: 0.1284 - val_loss: 0.1232\n",
      "Epoch 7/15\n",
      "60/60 [==============================] - 27s 449ms/step - loss: 0.1025 - val_loss: 0.1064\n",
      "Epoch 8/15\n",
      "60/60 [==============================] - 28s 472ms/step - loss: 0.0893 - val_loss: 0.0963\n",
      "Epoch 9/15\n",
      "60/60 [==============================] - 28s 468ms/step - loss: 0.0948 - val_loss: 0.0998\n",
      "Epoch 10/15\n",
      "60/60 [==============================] - 28s 468ms/step - loss: 0.0820 - val_loss: 0.0889\n",
      "Epoch 11/15\n",
      "60/60 [==============================] - 27s 445ms/step - loss: 0.0747 - val_loss: 0.0841\n",
      "Epoch 12/15\n",
      "60/60 [==============================] - 27s 447ms/step - loss: 0.0705 - val_loss: 0.0802\n",
      "Epoch 13/15\n",
      "60/60 [==============================] - 28s 469ms/step - loss: 0.0693 - val_loss: 0.0799\n",
      "Epoch 14/15\n",
      "60/60 [==============================] - 28s 469ms/step - loss: 0.0679 - val_loss: 0.0774\n",
      "Epoch 15/15\n",
      "60/60 [==============================] - 28s 469ms/step - loss: 0.0655 - val_loss: 0.0740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 13:19:09.384121: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 919800000 exceeds 10% of free system memory.\n",
      "2022-05-30 13:19:10.479730: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 919800000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "60/60 [==============================] - 28s 458ms/step - loss: 3.4577 - Label_loss: 2.5802 - Domain_loss: 8.4493 - Label_acc: 0.6412 - Domain_acc: 0.1221 - val_loss: 1.7189 - val_Label_loss: 1.6771 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.6775 - val_Domain_acc: 0.0076\n",
      "Epoch 2/30\n",
      "60/60 [==============================] - 27s 455ms/step - loss: 1.1104 - Label_loss: 0.7211 - Domain_loss: 3.4713 - Label_acc: 0.7538 - Domain_acc: 0.1335 - val_loss: 0.5657 - val_Label_loss: 0.5233 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8139 - val_Domain_acc: 0.0043\n",
      "Epoch 3/30\n",
      "60/60 [==============================] - 27s 452ms/step - loss: 0.9564 - Label_loss: 0.6118 - Domain_loss: 3.0217 - Label_acc: 0.7710 - Domain_acc: 0.1453 - val_loss: 0.5033 - val_Label_loss: 0.4608 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8333 - val_Domain_acc: 0.2846\n",
      "Epoch 4/30\n",
      "60/60 [==============================] - 28s 472ms/step - loss: 0.8748 - Label_loss: 0.5595 - Domain_loss: 2.7296 - Label_acc: 0.7856 - Domain_acc: 0.1718 - val_loss: 0.4523 - val_Label_loss: 0.4099 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8474 - val_Domain_acc: 0.2532\n",
      "Epoch 5/30\n",
      "60/60 [==============================] - 29s 476ms/step - loss: 0.7785 - Label_loss: 0.4786 - Domain_loss: 2.5775 - Label_acc: 0.8119 - Domain_acc: 0.1936 - val_loss: 0.4718 - val_Label_loss: 0.4301 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8431 - val_Domain_acc: 0.1818\n",
      "Epoch 6/30\n",
      "60/60 [==============================] - 27s 457ms/step - loss: 0.7778 - Label_loss: 0.4899 - Domain_loss: 2.4640 - Label_acc: 0.8034 - Domain_acc: 0.2172 - val_loss: 0.4580 - val_Label_loss: 0.4166 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8496 - val_Domain_acc: 0.0595\n",
      "Epoch 7/30\n",
      "60/60 [==============================] - 29s 480ms/step - loss: 0.7521 - Label_loss: 0.4763 - Domain_loss: 2.3430 - Label_acc: 0.8102 - Domain_acc: 0.2432 - val_loss: 0.4456 - val_Label_loss: 0.4042 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8474 - val_Domain_acc: 0.1656\n",
      "Epoch 8/30\n",
      "60/60 [==============================] - 27s 455ms/step - loss: 0.7143 - Label_loss: 0.4463 - Domain_loss: 2.2682 - Label_acc: 0.8175 - Domain_acc: 0.2540 - val_loss: 0.4375 - val_Label_loss: 0.3965 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8528 - val_Domain_acc: 0.1212\n",
      "Epoch 9/30\n",
      "60/60 [==============================] - 28s 462ms/step - loss: 0.6921 - Label_loss: 0.4326 - Domain_loss: 2.1879 - Label_acc: 0.8223 - Domain_acc: 0.2718 - val_loss: 0.5649 - val_Label_loss: 0.5244 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8106 - val_Domain_acc: 0.0043\n",
      "Epoch 10/30\n",
      "60/60 [==============================] - 28s 459ms/step - loss: 0.6873 - Label_loss: 0.4352 - Domain_loss: 2.1134 - Label_acc: 0.8235 - Domain_acc: 0.2869 - val_loss: 0.5010 - val_Label_loss: 0.4599 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8387 - val_Domain_acc: 0.0714\n",
      "Epoch 11/30\n",
      "60/60 [==============================] - 28s 459ms/step - loss: 0.6674 - Label_loss: 0.4190 - Domain_loss: 2.0739 - Label_acc: 0.8304 - Domain_acc: 0.3025 - val_loss: 0.4568 - val_Label_loss: 0.4160 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8485 - val_Domain_acc: 0.0206\n",
      "Epoch 12/30\n",
      "60/60 [==============================] - 27s 457ms/step - loss: 0.6341 - Label_loss: 0.3920 - Domain_loss: 2.0131 - Label_acc: 0.8415 - Domain_acc: 0.3208 - val_loss: 0.6166 - val_Label_loss: 0.5758 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8095 - val_Domain_acc: 0.0238\n",
      "Epoch 13/30\n",
      "60/60 [==============================] - 28s 459ms/step - loss: 0.6555 - Label_loss: 0.4159 - Domain_loss: 1.9854 - Label_acc: 0.8297 - Domain_acc: 0.3109 - val_loss: 0.5213 - val_Label_loss: 0.4798 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8333 - val_Domain_acc: 0.0141\n",
      "Epoch 14/30\n",
      "60/60 [==============================] - 28s 459ms/step - loss: 0.6290 - Label_loss: 0.4025 - Domain_loss: 1.8503 - Label_acc: 0.8401 - Domain_acc: 0.3696 - val_loss: 0.5360 - val_Label_loss: 0.4945 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8225 - val_Domain_acc: 0.0054\n",
      "Epoch 15/30\n",
      "60/60 [==============================] - 27s 457ms/step - loss: 0.6278 - Label_loss: 0.3952 - Domain_loss: 1.9074 - Label_acc: 0.8429 - Domain_acc: 0.3517 - val_loss: 0.6187 - val_Label_loss: 0.5767 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8041 - val_Domain_acc: 0.0227\n",
      "Epoch 16/30\n",
      "60/60 [==============================] - 27s 457ms/step - loss: 0.6108 - Label_loss: 0.3860 - Domain_loss: 1.8283 - Label_acc: 0.8428 - Domain_acc: 0.3813 - val_loss: 0.5363 - val_Label_loss: 0.4946 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8452 - val_Domain_acc: 0.0087\n",
      "Epoch 17/30\n",
      "60/60 [==============================] - 27s 455ms/step - loss: 0.6157 - Label_loss: 0.3928 - Domain_loss: 1.8116 - Label_acc: 0.8414 - Domain_acc: 0.3905 - val_loss: 0.7737 - val_Label_loss: 0.7317 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.7565 - val_Domain_acc: 0.0563\n",
      "Epoch 18/30\n",
      "60/60 [==============================] - 28s 472ms/step - loss: 0.6046 - Label_loss: 0.3854 - Domain_loss: 1.7698 - Label_acc: 0.8458 - Domain_acc: 0.3992 - val_loss: 0.6392 - val_Label_loss: 0.5970 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.7965 - val_Domain_acc: 0.0000e+00\n",
      "Epoch 19/30\n",
      "60/60 [==============================] - 28s 472ms/step - loss: 0.5925 - Label_loss: 0.3762 - Domain_loss: 1.7405 - Label_acc: 0.8463 - Domain_acc: 0.4034 - val_loss: 0.5747 - val_Label_loss: 0.5323 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8496 - val_Domain_acc: 0.0325\n",
      "Epoch 20/30\n",
      "60/60 [==============================] - 28s 472ms/step - loss: 0.5855 - Label_loss: 0.3727 - Domain_loss: 1.7001 - Label_acc: 0.8536 - Domain_acc: 0.4240 - val_loss: 0.4951 - val_Label_loss: 0.4525 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8398 - val_Domain_acc: 0.0325\n",
      "Epoch 21/30\n",
      "60/60 [==============================] - 28s 472ms/step - loss: 0.5771 - Label_loss: 0.3636 - Domain_loss: 1.7059 - Label_acc: 0.8544 - Domain_acc: 0.4176 - val_loss: 0.5087 - val_Label_loss: 0.4655 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8506 - val_Domain_acc: 0.0076\n",
      "Epoch 22/30\n",
      "60/60 [==============================] - 27s 452ms/step - loss: 0.5808 - Label_loss: 0.3690 - Domain_loss: 1.6827 - Label_acc: 0.8500 - Domain_acc: 0.4316 - val_loss: 0.4769 - val_Label_loss: 0.4334 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8571 - val_Domain_acc: 0.0509\n",
      "Epoch 23/30\n",
      "60/60 [==============================] - 28s 471ms/step - loss: 0.5528 - Label_loss: 0.3500 - Domain_loss: 1.5922 - Label_acc: 0.8635 - Domain_acc: 0.4556 - val_loss: 0.5533 - val_Label_loss: 0.5098 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8160 - val_Domain_acc: 0.0173\n",
      "Epoch 24/30\n",
      "60/60 [==============================] - 28s 470ms/step - loss: 0.5466 - Label_loss: 0.3413 - Domain_loss: 1.6191 - Label_acc: 0.8611 - Domain_acc: 0.4573 - val_loss: 0.5663 - val_Label_loss: 0.5230 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8203 - val_Domain_acc: 0.0314\n",
      "Epoch 25/30\n",
      "60/60 [==============================] - 28s 472ms/step - loss: 0.5575 - Label_loss: 0.3547 - Domain_loss: 1.5889 - Label_acc: 0.8564 - Domain_acc: 0.4660 - val_loss: 0.5832 - val_Label_loss: 0.5387 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8128 - val_Domain_acc: 0.0119\n",
      "Epoch 26/30\n",
      "60/60 [==============================] - 28s 472ms/step - loss: 0.5484 - Label_loss: 0.3476 - Domain_loss: 1.5601 - Label_acc: 0.8612 - Domain_acc: 0.4821 - val_loss: 0.5153 - val_Label_loss: 0.4703 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8420 - val_Domain_acc: 0.0097\n",
      "Epoch 27/30\n",
      "60/60 [==============================] - 27s 453ms/step - loss: 0.5369 - Label_loss: 0.3355 - Domain_loss: 1.5620 - Label_acc: 0.8626 - Domain_acc: 0.4795 - val_loss: 0.4725 - val_Label_loss: 0.4274 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8312 - val_Domain_acc: 0.0065\n",
      "Epoch 28/30\n",
      "60/60 [==============================] - 28s 471ms/step - loss: 0.5445 - Label_loss: 0.3435 - Domain_loss: 1.5577 - Label_acc: 0.8644 - Domain_acc: 0.4797 - val_loss: 0.8893 - val_Label_loss: 0.8437 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.7348 - val_Domain_acc: 0.0054\n",
      "Epoch 29/30\n",
      "60/60 [==============================] - 27s 451ms/step - loss: 0.5595 - Label_loss: 0.3560 - Domain_loss: 1.5751 - Label_acc: 0.8544 - Domain_acc: 0.4808 - val_loss: 0.6059 - val_Label_loss: 0.5596 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8095 - val_Domain_acc: 0.0130\n",
      "Epoch 30/30\n",
      "60/60 [==============================] - 27s 450ms/step - loss: 0.5246 - Label_loss: 0.3239 - Domain_loss: 1.5457 - Label_acc: 0.8672 - Domain_acc: 0.4941 - val_loss: 0.5624 - val_Label_loss: 0.5162 - val_Domain_loss: 0.0000e+00 - val_Label_acc: 0.8139 - val_Domain_acc: 0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 13:33:33.607910: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 919800000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(924, 10, 128)\n",
      "Fold # 1\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 3000, 16)     416         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 3000, 16)     64          conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 3000, 16)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1500, 16)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1500, 16)     6416        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1500, 16)     64          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1500, 16)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1500, 16)     32          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 750, 16)      0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 750, 16)      272         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 750, 16)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 750, 16)      0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 750, 32)      12832       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 750, 32)      128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 750, 32)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 375, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 375, 32)      25632       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 375, 32)      128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 375, 32)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 375, 32)      544         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 188, 32)      0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 188, 32)      1056        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 188, 32)      0           max_pooling1d_3[0][0]            \n",
      "                                                                 conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 188, 32)      0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 188, 64)      51264       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 188, 64)      256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 188, 64)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 94, 64)       0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 94, 64)       102464      max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 94, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 94, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 94, 64)       2112        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 47, 64)       0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 47, 64)       4160        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 47, 64)       0           max_pooling1d_5[0][0]            \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 47, 64)       0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 47, 128)      204928      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 47, 128)      512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 47, 128)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 24, 128)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 24, 128)      409728      max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 24, 128)      512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 24, 128)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 24, 128)      8320        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 12, 128)      0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 12, 128)      16512       conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 12, 128)      0           max_pooling1d_7[0][0]            \n",
      "                                                                 conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 848,608\n",
      "Trainable params: 847,648\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n",
      "(911, 10, 128)\n",
      "Fold # 2\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 3000, 16)     416         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 3000, 16)     64          conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 3000, 16)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1500, 16)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1500, 16)     6416        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1500, 16)     64          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1500, 16)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1500, 16)     32          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 750, 16)      0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 750, 16)      272         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 750, 16)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 750, 16)      0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 750, 32)      12832       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 750, 32)      128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 750, 32)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 375, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 375, 32)      25632       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 375, 32)      128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 375, 32)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 375, 32)      544         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 188, 32)      0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 188, 32)      1056        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 188, 32)      0           max_pooling1d_3[0][0]            \n",
      "                                                                 conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 188, 32)      0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 188, 64)      51264       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 188, 64)      256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 188, 64)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 94, 64)       0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 94, 64)       102464      max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 94, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 94, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 94, 64)       2112        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 47, 64)       0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 47, 64)       4160        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 47, 64)       0           max_pooling1d_5[0][0]            \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 47, 64)       0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 47, 128)      204928      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 47, 128)      512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 47, 128)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 24, 128)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 24, 128)      409728      max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 24, 128)      512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 24, 128)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 24, 128)      8320        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 12, 128)      0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 12, 128)      16512       conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 12, 128)      0           max_pooling1d_7[0][0]            \n",
      "                                                                 conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 848,608\n",
      "Trainable params: 847,648\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n",
      "(794, 10, 128)\n",
      "Fold # 3\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 3000, 16)     416         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 3000, 16)     64          conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 3000, 16)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1500, 16)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1500, 16)     6416        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1500, 16)     64          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1500, 16)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1500, 16)     32          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 750, 16)      0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 750, 16)      272         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 750, 16)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 750, 16)      0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 750, 32)      12832       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 750, 32)      128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 750, 32)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 375, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 375, 32)      25632       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 375, 32)      128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 375, 32)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 375, 32)      544         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 188, 32)      0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 188, 32)      1056        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 188, 32)      0           max_pooling1d_3[0][0]            \n",
      "                                                                 conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 188, 32)      0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 188, 64)      51264       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 188, 64)      256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 188, 64)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 94, 64)       0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 94, 64)       102464      max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 94, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 94, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 94, 64)       2112        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 47, 64)       0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 47, 64)       4160        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 47, 64)       0           max_pooling1d_5[0][0]            \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 47, 64)       0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 47, 128)      204928      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 47, 128)      512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 47, 128)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 24, 128)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 24, 128)      409728      max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 24, 128)      512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 24, 128)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 24, 128)      8320        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 12, 128)      0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 12, 128)      16512       conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 12, 128)      0           max_pooling1d_7[0][0]            \n",
      "                                                                 conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 848,608\n",
      "Trainable params: 847,648\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n",
      "(764, 10, 128)\n",
      "Fold # 4\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 3000, 16)     416         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 3000, 16)     64          conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 3000, 16)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1500, 16)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1500, 16)     6416        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1500, 16)     64          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1500, 16)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1500, 16)     32          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 750, 16)      0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 750, 16)      272         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 750, 16)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 750, 16)      0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 750, 32)      12832       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 750, 32)      128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 750, 32)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 375, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 375, 32)      25632       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 375, 32)      128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 375, 32)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 375, 32)      544         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 188, 32)      0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 188, 32)      1056        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 188, 32)      0           max_pooling1d_3[0][0]            \n",
      "                                                                 conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 188, 32)      0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 188, 64)      51264       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 188, 64)      256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 188, 64)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 94, 64)       0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 94, 64)       102464      max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 94, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 94, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 94, 64)       2112        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 47, 64)       0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 47, 64)       4160        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 47, 64)       0           max_pooling1d_5[0][0]            \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 47, 64)       0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 47, 128)      204928      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 47, 128)      512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 47, 128)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 24, 128)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 24, 128)      409728      max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 24, 128)      512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 24, 128)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 24, 128)      8320        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 12, 128)      0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 12, 128)      16512       conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 12, 128)      0           max_pooling1d_7[0][0]            \n",
      "                                                                 conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 848,608\n",
      "Trainable params: 847,648\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n",
      "(914, 10, 128)\n",
      "Fold # 5\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 3000, 16)     416         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 3000, 16)     64          conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 3000, 16)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1500, 16)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1500, 16)     6416        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1500, 16)     64          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1500, 16)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1500, 16)     32          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 750, 16)      0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 750, 16)      272         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 750, 16)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 750, 16)      0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 750, 32)      12832       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 750, 32)      128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 750, 32)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 375, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 375, 32)      25632       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 375, 32)      128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 375, 32)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 375, 32)      544         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 188, 32)      0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 188, 32)      1056        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 188, 32)      0           max_pooling1d_3[0][0]            \n",
      "                                                                 conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 188, 32)      0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 188, 64)      51264       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 188, 64)      256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 188, 64)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 94, 64)       0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 94, 64)       102464      max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 94, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 94, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 94, 64)       2112        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 47, 64)       0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 47, 64)       4160        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 47, 64)       0           max_pooling1d_5[0][0]            \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 47, 64)       0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 47, 128)      204928      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 47, 128)      512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 47, 128)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 24, 128)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 24, 128)      409728      max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 24, 128)      512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 24, 128)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 24, 128)      8320        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 12, 128)      0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 12, 128)      16512       conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 12, 128)      0           max_pooling1d_7[0][0]            \n",
      "                                                                 conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 848,608\n",
      "Trainable params: 847,648\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 15:01:22.979770: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.00GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-05-30 15:01:23.112281: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.00GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(823, 10, 128)\n",
      "Fold # 6\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 3000, 16)     416         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 3000, 16)     64          conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 3000, 16)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1500, 16)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1500, 16)     6416        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1500, 16)     64          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1500, 16)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1500, 16)     32          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 750, 16)      0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 750, 16)      272         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 750, 16)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 750, 16)      0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 750, 32)      12832       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 750, 32)      128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 750, 32)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 375, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 375, 32)      25632       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 375, 32)      128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 375, 32)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 375, 32)      544         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 188, 32)      0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 188, 32)      1056        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 188, 32)      0           max_pooling1d_3[0][0]            \n",
      "                                                                 conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 188, 32)      0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 188, 64)      51264       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 188, 64)      256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 188, 64)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 94, 64)       0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 94, 64)       102464      max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 94, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 94, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 94, 64)       2112        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 47, 64)       0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 47, 64)       4160        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 47, 64)       0           max_pooling1d_5[0][0]            \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 47, 64)       0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 47, 128)      204928      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 47, 128)      512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 47, 128)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 24, 128)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 24, 128)      409728      max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 24, 128)      512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 24, 128)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 24, 128)      8320        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 12, 128)      0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 12, 128)      16512       conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 12, 128)      0           max_pooling1d_7[0][0]            \n",
      "                                                                 conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 848,608\n",
      "Trainable params: 847,648\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n",
      "(784, 10, 128)\n",
      "Fold # 7\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 3000, 16)     416         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 3000, 16)     64          conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 3000, 16)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1500, 16)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1500, 16)     6416        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1500, 16)     64          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1500, 16)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1500, 16)     32          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 750, 16)      0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 750, 16)      272         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 750, 16)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 750, 16)      0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 750, 32)      12832       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 750, 32)      128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 750, 32)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 375, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 375, 32)      25632       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 375, 32)      128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 375, 32)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 375, 32)      544         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 188, 32)      0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 188, 32)      1056        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 188, 32)      0           max_pooling1d_3[0][0]            \n",
      "                                                                 conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 188, 32)      0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 188, 64)      51264       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 188, 64)      256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 188, 64)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 94, 64)       0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 94, 64)       102464      max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 94, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 94, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 94, 64)       2112        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 47, 64)       0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 47, 64)       4160        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 47, 64)       0           max_pooling1d_5[0][0]            \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 47, 64)       0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 47, 128)      204928      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 47, 128)      512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 47, 128)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 24, 128)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 24, 128)      409728      max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 24, 128)      512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 24, 128)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 24, 128)      8320        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 12, 128)      0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 12, 128)      16512       conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 12, 128)      0           max_pooling1d_7[0][0]            \n",
      "                                                                 conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 848,608\n",
      "Trainable params: 847,648\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 15:45:55.921529: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.23GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-05-30 15:45:56.035625: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.23GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-05-30 15:45:58.362312: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-05-30 16:06:13.603771: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.17GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(970, 10, 128)\n",
      "Fold # 8\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 3000, 16)     416         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 3000, 16)     64          conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 3000, 16)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1500, 16)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1500, 16)     6416        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1500, 16)     64          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1500, 16)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1500, 16)     32          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 750, 16)      0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 750, 16)      272         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 750, 16)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 750, 16)      0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 750, 32)      12832       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 750, 32)      128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 750, 32)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 375, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 375, 32)      25632       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 375, 32)      128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 375, 32)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 375, 32)      544         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 188, 32)      0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 188, 32)      1056        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 188, 32)      0           max_pooling1d_3[0][0]            \n",
      "                                                                 conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 188, 32)      0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 188, 64)      51264       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 188, 64)      256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 188, 64)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 94, 64)       0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 94, 64)       102464      max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 94, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 94, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 94, 64)       2112        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 47, 64)       0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 47, 64)       4160        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 47, 64)       0           max_pooling1d_5[0][0]            \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 47, 64)       0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 47, 128)      204928      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 47, 128)      512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 47, 128)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 24, 128)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 24, 128)      409728      max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 24, 128)      512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 24, 128)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 24, 128)      8320        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 12, 128)      0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 12, 128)      16512       conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 12, 128)      0           max_pooling1d_7[0][0]            \n",
      "                                                                 conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 848,608\n",
      "Trainable params: 847,648\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 16:06:48.417363: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.94GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-05-30 16:26:59.390902: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.48GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(939, 10, 128)\n",
      "Fold # 9\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 3000, 16)     416         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 3000, 16)     64          conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 3000, 16)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1500, 16)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1500, 16)     6416        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1500, 16)     64          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1500, 16)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1500, 16)     32          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 750, 16)      0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 750, 16)      272         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 750, 16)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 750, 16)      0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 750, 32)      12832       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 750, 32)      128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 750, 32)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 375, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 375, 32)      25632       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 375, 32)      128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 375, 32)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 375, 32)      544         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 188, 32)      0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 188, 32)      1056        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 188, 32)      0           max_pooling1d_3[0][0]            \n",
      "                                                                 conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 188, 32)      0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 188, 64)      51264       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 188, 64)      256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 188, 64)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 94, 64)       0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 94, 64)       102464      max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 94, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 94, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 94, 64)       2112        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 47, 64)       0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 47, 64)       4160        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 47, 64)       0           max_pooling1d_5[0][0]            \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 47, 64)       0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 47, 128)      204928      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 47, 128)      512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 47, 128)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 24, 128)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 24, 128)      409728      max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 24, 128)      512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 24, 128)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 24, 128)      8320        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 12, 128)      0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 12, 128)      16512       conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 12, 128)      0           max_pooling1d_7[0][0]            \n",
      "                                                                 conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 848,608\n",
      "Trainable params: 847,648\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 16:27:32.193423: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.41GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-05-30 16:27:32.211069: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(766, 10, 128)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "# k-fold cross validation\n",
    "all_scores = []\n",
    "\n",
    "cfg = {\n",
    "    'bs':128,\n",
    "    'epochs': 50\n",
    "}\n",
    "first_decay_steps = 10\n",
    "lr_decayed_fn = (\n",
    "  tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "      0.001,\n",
    "      first_decay_steps))\n",
    "\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "for i in range(0, 10):\n",
    "    print('Fold #', i)\n",
    "    train_data,train_targets,val_data,val_targets = DataGenerator.getFold(i)\n",
    "    train_data, val_data = train_data.reshape(-1, 3000, 6+4), val_data.reshape(-1, 3000, 6+4)\n",
    "    train_domin, val_domin = Dom_Generator.getFold(i)\n",
    "#     with strategy.scope():\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "    fea_model, cl_model, ce_model, pre_model = create_model(train_data.shape[1:])\n",
    "    cl_model.compile(optimizer=opt, \n",
    "                  loss=multi_label_supconloss)\n",
    "    verbose=0\n",
    "    if i == 0:\n",
    "        verbose=1\n",
    "    history = cl_model.fit(train_data, train_targets, \n",
    "                    batch_size=cfg['bs'], epochs=15, \n",
    "                    validation_data=(val_data, val_targets),\n",
    "                    verbose=verbose)\n",
    "#     with strategy.scope():\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "    ce_model.compile(optimizer=opt, \n",
    "                     loss={'Label': \"categorical_crossentropy\", 'Domain': \"categorical_crossentropy\"}, \n",
    "                     metrics={'Label': \"acc\", 'Domain': \"acc\"}, \n",
    "                     loss_weights=[1,0.1]\n",
    "                  )\n",
    "    pre_model.compile(optimizer=opt, \n",
    "                  loss=\"categorical_crossentropy\", \n",
    "                  metrics=['acc'], \n",
    "                  )\n",
    "    \n",
    "    history = ce_model.fit(train_data, [train_targets, train_domin, train_targets], \n",
    "                           batch_size=cfg['bs'], epochs=30, \n",
    "                           validation_data=(val_data, [val_targets, val_domin, val_targets]),\n",
    "                           callbacks=[tf.keras.callbacks.ModelCheckpoint(str(i)+'ResNet_Best'+'.h5', \n",
    "                                                   monitor='val_Label_acc', \n",
    "                                                   verbose=0, \n",
    "                                                   save_best_only=True, \n",
    "                                                   save_weights_only=True, \n",
    "                                                   mode='auto', \n",
    "                                                   period=1 )],\n",
    "                           verbose=verbose)\n",
    "    # get and save the learned feature\n",
    "    ce_model.load_weights(str(i)+'ResNet_Best'+'.h5')\n",
    "    train_feature = fea_model.predict(train_data)\n",
    "    val_feature = fea_model.predict(val_data)\n",
    "    print(val_feature.shape)\n",
    "    np.savez('Feature_'+str(i)+'.npz', \n",
    "        train_feature = train_feature,\n",
    "        val_feature = val_feature,\n",
    "        train_targets = train_targets,\n",
    "        val_targets = val_targets\n",
    "    )\n",
    "    keras.backend.clear_session()\n",
    "    del train_data,train_targets,val_data,val_targets, fea_model, cl_model, train_domin, val_domin\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9ec4a06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T16:48:35.769731Z",
     "iopub.status.busy": "2022-05-30T16:48:35.768919Z",
     "iopub.status.idle": "2022-05-30T16:49:33.954440Z",
     "shell.execute_reply": "2022-05-30T16:49:33.953690Z"
    },
    "papermill": {
     "duration": 59.585686,
     "end_time": "2022-05-30T16:49:33.956326",
     "exception": false,
     "start_time": "2022-05-30T16:48:34.370640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 0.8571428656578064\n",
      "Evaluate 0.8221734166145325\n",
      "Evaluate 0.8438287377357483\n",
      "Evaluate 0.7526177763938904\n",
      "Evaluate 0.863238513469696\n",
      "Evaluate 0.8153098225593567\n",
      "Evaluate 0.8571428656578064\n",
      "Evaluate 0.907216489315033\n",
      "Evaluate 0.813631534576416\n",
      "Evaluate 0.8015666007995605\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for i in range(10):\n",
    "    train_data,train_targets,val_data,val_targets = DataGenerator.getFold(i)\n",
    "    val_data = val_data.reshape(-1, 3000, 6+4)\n",
    "    ce_model.load_weights(str(i)+'ResNet_Best'+'.h5')\n",
    "    val_mse, val_acc = pre_model.evaluate(val_data, val_targets, verbose=0)\n",
    "    res.append(val_acc)\n",
    "    print('Evaluate', val_acc)\n",
    "    del train_data,train_targets,val_data,val_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdb2024e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T16:49:35.781816Z",
     "iopub.status.busy": "2022-05-30T16:49:35.781561Z",
     "iopub.status.idle": "2022-05-30T16:49:35.787231Z",
     "shell.execute_reply": "2022-05-30T16:49:35.786560Z"
    },
    "papermill": {
     "duration": 0.921113,
     "end_time": "2022-05-30T16:49:35.789400",
     "exception": false,
     "start_time": "2022-05-30T16:49:34.868287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333868622779846"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(res).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5b28d1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T16:49:37.640888Z",
     "iopub.status.busy": "2022-05-30T16:49:37.640558Z",
     "iopub.status.idle": "2022-05-30T16:49:37.648340Z",
     "shell.execute_reply": "2022-05-30T16:49:37.647525Z"
    },
    "papermill": {
     "duration": 0.945721,
     "end_time": "2022-05-30T16:49:37.652041",
     "exception": false,
     "start_time": "2022-05-30T16:49:36.706320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8571428656578064,\n",
       " 0.8221734166145325,\n",
       " 0.8438287377357483,\n",
       " 0.7526177763938904,\n",
       " 0.863238513469696,\n",
       " 0.8153098225593567,\n",
       " 0.8571428656578064,\n",
       " 0.907216489315033,\n",
       " 0.813631534576416,\n",
       " 0.8015666007995605]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13141.713197,
   "end_time": "2022-05-30T16:49:41.830630",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-30T13:10:40.117433",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}